{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/skillfactorylogo.png\"></center>\n",
    "\n",
    "<h1><center>Курс \"Практический Machine Learning\"</center></h1>\n",
    "<h3><center>Шестаков Андрей</center></h3>\n",
    "<hr>\n",
    "<h2><center>Композиции алгоритмов</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aka \n",
    "* Ансамбли моделей\n",
    "* Комитеты алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from ipywidgets import interact, IntSlider\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Довольно популярный подход на соревнованиях (и все чаще - в реальных приложениях) смысл которого заключается в объединении предсказаний нескольких разных моделей.\n",
    "\n",
    "## Пример из идеального мира:\n",
    "\n",
    "* Пусть решается задача бинарной классификации\n",
    "* Пусть у нас есть $M$ независимых *базовых* моделей, которые выдают верное предсказание с вероятностью $p \\in (1/2, 1]$\n",
    "* Финальный ответ определяется простым большинством\n",
    "* Оказывается, что чем больше классификаторов в ансамбле - тем больше вероятность угадать ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100\n",
    "T = M/2 + 1\n",
    "p = 0.6\n",
    "p_ens = sum([scipy.special.comb(M,k)*(p**k)*((1-p)**(M-k)) for k in range(T, M+1)])\n",
    "p_ens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понятное дело, что на практике допольно сложно совместить идеальную независимость моделей и преемлемое качество, но мы можем постараться:\n",
    "\n",
    "* Строить **базовые** модели на разных признаках\n",
    "* Строить **базовые** модели на *немного разных* выборках"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging и Случайный лес (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Дерево решений очень чувствительно к данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "def demo_2dec_tree():\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.set_figheight(5)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    X, y = make_moons(noise=0.3, random_state=42)\n",
    "    \n",
    "    # Dec Tree Stuff\n",
    "    for i in range(2):\n",
    "        idx = np.random.randint(0, X.shape[0], int(0.95*X.shape[0]))\n",
    "\n",
    "        X1 = X[idx, :]\n",
    "        y1 = y[idx]\n",
    "\n",
    "        ax[i].scatter(X1[:,0], X1[:, 1], c=y1)\n",
    "        ax[i].set_xlabel('$x_1$')\n",
    "        ax[i].set_ylabel('$x_2$')\n",
    "\n",
    "        # Dec Tree Stuff 2\n",
    "        tree = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=123)\n",
    "        tree.fit(X1,y1)\n",
    "\n",
    "        x_range = np.linspace(X.min(), X.max(), 100)\n",
    "        xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "\n",
    "        Y = tree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "        Y = Y.reshape(xx1.shape)\n",
    "\n",
    "        ax[i].contourf(xx1, xx2, Y, alpha=0.3)\n",
    "        ax[i].scatter(X[:,0], X[:,1],c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "demo_2dec_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bagging - это **параллельный** способ построения ансамбля.<br/>\n",
    "1. Обучающая выборка сэмплируется $k$ раз с помощью *bootstrap'a* (выборка с возвратом)\n",
    "2. На каждом сэмпле обучается отдельная **базовая модель**\n",
    "3. Ответы моделей усредняются (возможно с весом)\n",
    "<center><img src='http://image.slidesharecdn.com/ipbimprovingthemodelspredictivepowerwithensembleapproaches-121203224610-phpapp02/95/improving-the-models-predictive-power-with-ensemble-approaches-10-638.jpg?cb=1354575467' width='750'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Так же есть некоторые обобщения бэггинга:\n",
    "\n",
    "* Метод случайных подпространств - на шаге 1. сэмплируются не только объекты, но и подпространство признаков\n",
    "* Метод случайного леса - на каждом узле сэмплируется подпространство признаков\n",
    "\n",
    "В данном случае, на каждом сэмпле базовой моделью является дерево решений.<br/>\n",
    "Если вам нужно за минимальное время построить достаточно точную и устойчивую модель - это ваш вариант.\n",
    "\n",
    "\n",
    "<center><img src='img/rf-quote.png'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rf_demo(n_est=5):\n",
    "    rf = RandomForestClassifier(random_state=123, n_estimators=n_est)\n",
    "\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    X, y = make_moons(noise=0.3, random_state=123)\n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    x_range = np.linspace(X.min(), X.max(), 100)\n",
    "    xx1, xx2 = np.meshgrid(x_range, x_range)\n",
    "    \n",
    "    \n",
    "    for tree in rf.estimators_:\n",
    "        y_hat = tree.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "        y_hat = y_hat.reshape(xx1.shape)\n",
    "\n",
    "        plt.contourf(xx1, xx2, y_hat, alpha=1.0/n_est)\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('$x_2$')\n",
    "    \n",
    "    plt.title('N estimators = %d' % n_est)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = interact(rf_demo, n_est=IntSlider(min=1, max=101, value=1, step=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Данные об оттоке из игры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = pd.read_csv('data/x_train.csv', sep=';')\n",
    "dfy = pd.read_csv('data/y_train.csv', names=['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfx.values\n",
    "y = dfy.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Качество на одном дереве"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=4, random_state=123)\n",
    "scoring = 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = range(1, 10)\n",
    "_, tree_test_scores = validation_curve(model, X, y, param_name='max_depth', param_range=depths, \n",
    "                 scoring=scoring, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depths, tree_test_scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бэггинг над деревьями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, bag_test_scores = validation_curve(model, X, y, param_name='base_estimator__max_depth', param_range=depths, \n",
    "                 scoring=scoring, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depths, bag_test_scores.mean(axis=1), label='bag')\n",
    "plt.plot(depths, tree_test_scores.mean(axis=1), label='tree')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Случайных лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=123, n_estimators=50, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, forest_test_scores = validation_curve(model, X, y, param_name='max_depth', param_range=depths, \n",
    "                 scoring=scoring, cv=cv, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depths, bag_test_scores.mean(axis=1), label='bag')\n",
    "plt.plot(depths, tree_test_scores.mean(axis=1), label='tree')\n",
    "plt.plot(depths, forest_test_scores.mean(axis=1), label='forest')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разные фишки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Бэггинг можно делать не только над деревьями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=4, random_state=123)\n",
    "scoring = 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "n_est =  [1, 10, 20, 50, 100]\n",
    "\n",
    "for n in n_est:\n",
    "    model = BaggingClassifier(base_estimator=LogisticRegression(), \n",
    "                              n_estimators=n, \n",
    "                              bootstrap_features=False, n_jobs=1)\n",
    "    scores.append(cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Листья деревьев как признаки!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как применять линейную модель, можно собрать специальные признаки на основе деревьев.\n",
    "\n",
    "* Предварительно на данных строим лес из $T$ деревьем\n",
    "* Нумеруем листья каждого дерева от $0$ до $L_T$\n",
    "* Для каждого дерева собираем новый признак \"номер листа в котором оказался объект\"\n",
    "* Прогоняем новые признаки через OHE \n",
    "* Profit?\n",
    "\n",
    "Можно строить такое признаковое пространство с помощью случайного леса, но надо быть осторожным!\n",
    "\n",
    "Можно строить случайные деревья!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomTreesEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = RandomTreesEmbedding(n_estimators=10, max_depth=5, random_state=123)\n",
    "emb.fit(X)\n",
    "\n",
    "# emb = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=123)\n",
    "# emb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb = emb.apply(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb_sparse = emb.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tree_emb', RandomTreesEmbedding(n_estimators=50, max_depth=4)),\n",
    "    ('clf', LogisticRegression(C=0.1))\n",
    "])\n",
    "\n",
    "simple_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(C=0.1))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X, y, scoring=scoring, cv=cv)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(simple_model, X, y, scoring=scoring, cv=cv)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting, в отличие от bagging'а - это **последовательный** способ построения композиции моделей.\n",
    "\n",
    "Мы постоянно работаем с одним и тем же набором данных, **но** на каждом шаге строим новую базовую модель, которая учитывает ошибки предыдущей модели.<br\\>По большому счету, бустинг-алгоритмы отличаются лишь тем, как в них заложен учет этих самых ошибок.\n",
    "\n",
    "Например в методе **AdaBoost** каждому объекту присваивается вес, который изменяется в зависимости от того, ошиблась ли на нем очередная композиция базовых алгоритмов или нет. Так же веса имеются и у самих базовых моделей, которые штрафуют их за плохие предсказания. Для задачи классификации этот процесс можно проиллюстрировать следующим образом:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/adaboost.png' width='650'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формальное описание алгоритма AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обозначения\n",
    "Введем следующие обозначения:\n",
    "* $t_k$ - базовый классификатор, полученный на шаге $k$\n",
    "* $\\alpha_k$ - вес базового классификатора, полученного на шаге $k$\n",
    "* $w_k(i)$ - веса объектов на шаге $k$\n",
    "* $x_i$ - $i$-й объект, $i = 1, \\dots, N$\n",
    "* $y_i=\\{-1, 1\\}$ - метки класса для $i$-го объекта \n",
    "\n",
    "#### Конечное предсказание\n",
    "Конечное предсказание получается из взвешенной комбинации предсказания базовых моделей:\n",
    "$$ T(x^*) = sign(\\sum\\limits^{K}_{k=1}\\alpha_kt_k(x^*)) $$\n",
    "\n",
    "Наша цель - минимизировать количество ошибок на всей выборке ..\n",
    "\n",
    "$$ E_T = \\frac{1}{N}\\sum\\limits_{i=1}^N [T(x_i) \\neq y_i] $$\n",
    "\n",
    ".. которые мы мажорируем экспонентой =)\n",
    "\n",
    "$$ E_T = \\frac{1}{N}\\sum\\limits_{i=1}^N [T(x_i) \\neq y_i] \\leq \\frac{1}{N}\\sum\\limits_{i=1}^N e^{(-y_i\\sum_k\\alpha_kt_k(x_i))} $$\n",
    "\n",
    "Если мы посчитаем ошибки $E_1, E_2, E_3,...$ на каждом шаге, то это даст нам правило для обновления весов объектов. <br\\>\n",
    "А если мы посчитаем производную ошибки $E_t$ по $\\alpha_t$, то это даст нам правило для обновления весов базовых моделей. <br\\>\n",
    "\n",
    "#### Алгоритм\n",
    "Алгоритм обучения **Discrete AdaBoost**:\n",
    "\n",
    "* Инициализируем веса объектов $w_1(i) = \\frac{1}{N}$ \n",
    "* Для $k = 1..K$\n",
    "    * Обучить классификатор $t_k(x) \\in \\{-1, 1\\}$ используя веса объектов $w(i)_k$\n",
    "    * Вычислить взвешенную ошибку $\\epsilon = \\frac{\\sum_i w_{k}(i)[y_i \\neq t_k(x_i) ]}{\\sum_i w_{k}(i)}$\n",
    "    * Вычислить вес базовой модели $\\alpha_k = \\ln\\frac{1-\\epsilon}{\\epsilon}$\n",
    "    * Пересчитать веса объектов $w_{k+1}(i) = \\frac{w_{k}(i) e^{-\\alpha_k y_i t_k(x_i)}}{W}$, $i = 1, \\dots, N$,где $W = \\sum_i w_k(i) e^{-\\alpha_k y_i t_k(x_i)}$ - нормировочная константа.\n",
    "* $T(x) = sign(\\sum_k \\alpha_k t_k(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Демо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def ada_demo(n_est=1):\n",
    "\n",
    "    ada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1,), n_estimators=n_est, learning_rate=0.1)\n",
    "    ada.fit(X_moons, y_moons)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "\n",
    "    xx1, xx2 = np.meshgrid(np.arange(-1.5, 2.5, 0.1),\n",
    "                           np.arange(-1, 1.5, 0.1))\n",
    "\n",
    "    y_hat = ada.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "    \n",
    "    y_hat = y_hat.reshape(xx1.shape)\n",
    "\n",
    "    plt.title('iteration = %d' % n_est )\n",
    "    plt.contourf(xx1, xx2, y_hat, cmap=plt.cm.Paired)\n",
    "    plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons, y_moons = make_moons(noise=0.1)\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(ada_demo, n_est=IntSlider(min=1, max=150, value=1, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим на наших данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=123, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), \n",
    "                         algorithm='SAMME',\n",
    "                         n_estimators=100,\n",
    "                         learning_rate=1., \n",
    "                         random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По некоторым математическим соображениям, веса базовых моделей в `SAMME.R` равны `1.` и learning rate учитывается в весах объектов (см. [тут](https://stackoverflow.com/questions/31981453/why-estimator-weight-in-samme-r-adaboost-algorithm-is-set-to-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train = []\n",
    "scores_valid = []\n",
    "\n",
    "for y_pred in ada.staged_predict_proba(X_train):\n",
    "    scores_train.append(roc_auc_score(y_train, y_pred[:, 1]))\n",
    "    \n",
    "for y_pred in ada.staged_predict_proba(X_valid):\n",
    "    scores_valid.append(roc_auc_score(y_valid, y_pred[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores_train, label='train score')\n",
    "plt.plot(scores_valid, label='valid score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(ada, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Градиентный бустинг\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По своей структуре метод похож на алгоритм градиентного спуска - отсюда и такое название."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/golf-MSE.png' width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/golf-table.svg' width=700></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше - [здесь](http://explained.ai/gradient-boosting/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Превью Градиентного Бустинга](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Немного формальности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть дана дифференцируемая функция потерь $L(T_k(x), y)$ (для любой задачи - регрессии или классификации) <br\\>\n",
    "Функционал качества - $Q(T, y) = \\sum_iL(T_k(x_i), y_i) = \\sum_iL(T_{k-1}(x_i) + t_{k}(x_i), y_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "На секунду представим, что $t_{k}(x_i)$ - это просто вектор значений (значение для каждого объекта). \n",
    "\n",
    "Надо \"сдвинуть\" ответы предыдущей композиции $T_{k-1}$, так чтобы минимизировать функцию потерь (как в градиентном спуске)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Тогда задачу оптимизации $Q(T, y)$ можно решать простым градиентным методом:\n",
    "\n",
    "* $T_0$ - начальное приближение\n",
    "* Посчитаем градиент функции потерь $L$: $g_i = \\frac{\\partial L(T_{k-1}(x_i), y_i)}{\\partial T_{k-1}(x_i)}$, $i = 1, \\dots, N$ \n",
    "* $T_k = T_{k-1} - \\alpha g$ - делаем градиентный шаг\n",
    "\n",
    "\n",
    "Тогда $t_{k}(x) =  \\arg\\min\\limits_{t} \\sum\\limits_i(t_{k}(x_i) - g_i)^2$, не зависимо от функции потерь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Демо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def grad_demo(n_est=1):\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    X = np.random.uniform(-10, 10, 500)\n",
    "\n",
    "    y = np.sin(X)/X + np.random.normal(0, .1, 500)\n",
    "    plt.scatter(X, y)\n",
    "    \n",
    "\n",
    "    gbr = GradientBoostingRegressor(n_estimators=n_est, learning_rate=0.15)\n",
    "    gbr_full = GradientBoostingRegressor(n_estimators=200, learning_rate=0.15)\n",
    "    gbr.fit(X.reshape(-1,1), y)\n",
    "    gbr_full.fit(X.reshape(-1,1), y)\n",
    "    \n",
    "    x_range = np.linspace(X.min(), X.max(), 100).reshape((-1,1))\n",
    "\n",
    "    for y_hat in gbr.staged_predict(x_range):\n",
    "        plt.plot(x_range, y_hat, alpha=0.4, c='g')\n",
    "\n",
    "    y_hat = gbr_full.predict(x_range)\n",
    "    \n",
    "    plt.title('Estimators %d' % n_est)\n",
    "    plt.plot(x_range, y_hat, c='r')\n",
    "    plt.xlabel('x')\n",
    "    plt.xlabel('y')\n",
    "    plt.ylim((-0.5, 1.3))\n",
    "    plt.xlim(-11,11)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "interact(grad_demo, n_est=IntSlider(min=1, max=150, value=1, step=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Особенности\n",
    "* Последовательное обучение\n",
    "* Склонность к переобучению\n",
    "* Проблемы с интерпретацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Наибольший успех, а потому и популярность, получил градиентный бустинг на деревьях решений.\n",
    "\n",
    "На досуге обратите внимание на модель `xgboost`. Вводный курс по этой модели можно найти [тут](http://education.parrotprediction.teachable.com/p/practical-xgboost-in-python)\n",
    "\n",
    "Так же есть другая популярная реализация бустинга [LightGBM](https://github.com/Microsoft/LightGBM)\n",
    "\n",
    "И, конечно же, недавний [CatBoost](https://tech.yandex.ru/catboost/), [видео](https://youtu.be/Q_xa4RvnDcY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "for lr in [0.02, 0.5, 1]:\n",
    "\n",
    "    model = GradientBoostingClassifier(n_estimators=300, max_depth=3,\n",
    "                                       learning_rate=lr, \n",
    "                                       random_state=123)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    scores_train = []\n",
    "    scores_valid = []\n",
    "\n",
    "    for y_pred in model.staged_predict_proba(X_train):\n",
    "        scores_train.append(roc_auc_score(y_train, y_pred[:, 1]))\n",
    "        \n",
    "    ax[0].plot(scores_train, label='lr={}'.format(lr))\n",
    "    ax[0].set_title('Train set')\n",
    "\n",
    "    for y_pred in model.staged_predict_proba(X_valid):\n",
    "        scores_valid.append(roc_auc_score(y_valid, y_pred[:, 1])) \n",
    "        \n",
    "    ax[1].plot(scores_valid, label='lr={}'.format(lr))\n",
    "    ax[1].set_title('Validation set')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=300, max_depth=3,\n",
    "                                       learning_rate=0.02, \n",
    "                                       random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = pd.Series(index=dfx.columns, data=model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попытка интерпретации - partial dependency plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =\\\n",
    "plot_partial_dependence(model, X_train, [0, 1, 3, (0,3)], \n",
    "                        feature_names=dfx.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp, axes = partial_dependence(model, [0], X=X_train, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(axes[0], pdp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Стэкинг и Блендинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схемы бэггинга и бустинка не гибкие. По сути, на каждом этапе обучается один и тот же алгоритм на немного измененных данных.\n",
    "\n",
    "Можно еще собирать ансамбли **разных моделей** и аггрегировать их результаты. Только не переобучиться бы.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Схема классического блендинга\n",
    "<center><img src='https://alexanderdyakonov.files.wordpress.com/2017/03/stacking.png?w=700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Схема классического стекинга\n",
    "<center><img src='https://alexanderdyakonov.files.wordpress.com/2017/03/stacking-2b.png?w=700'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стэкинг франкенштейн\n",
    "\n",
    "<img src='https://kaggle2.blob.core.windows.net/forum-message-attachments/79598/2514/FINAL_ARCHITECTURE.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vecstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=50, max_depth=5, random_state=123),\n",
    "    RandomForestClassifier(n_estimators=50, max_depth=10, random_state=123, max_features='log2'),\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ]),\n",
    "    Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=11))\n",
    "    ]),\n",
    "    GaussianNB()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_score_cust(y_true, y_hat):\n",
    "    return roc_auc_score(y_true, y_hat[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train, S_valid = stacking(models,\n",
    "                               X_train, y_train, X_valid,\n",
    "                               regression=False,\n",
    "                               mode='oof_pred_bag', \n",
    "                               needs_proba=True,\n",
    "                               metric=roc_auc_score_cust,\n",
    "                               n_folds=5,                \n",
    "                               stratified=True,          \n",
    "                               shuffle=True,             \n",
    "                               random_state=123,         \n",
    "                               verbose=2)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model = GradientBoostingClassifier(n_estimators=300, max_depth=3,\n",
    "                                       learning_rate=0.01, \n",
    "                                       random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_model.fit(S_train, y_train)\n",
    "y_hat = last_model.predict_proba(S_valid)\n",
    "roc_auc_score(y_valid, y_hat[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict_proba(X_valid)\n",
    "    score = roc_auc_score(y_valid, y_hat[:, 1])\n",
    "    print('{}: {}'.format(model.__str__().split('(')[0], score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Идея для ансамблей с временными рядами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo('yx6OQ-8HofU', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полезные ссылки\n",
    "\n",
    "* [Вводный курс по XGB](http://education.parrotprediction.teachable.com/p/practical-xgboost-in-python)\n",
    "* [LightGBM](https://github.com/Microsoft/LightGBM)\n",
    "* [CatBoost](https://tech.yandex.ru/catboost/)\n",
    "* [Сравнение библиотек](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n",
    "* [Kaggle про смешивание алгоритмов](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)\n",
    "* [Пост про градиентный бустинг от А.Дьяконова](https://alexanderdyakonov.wordpress.com/2017/06/09/%d0%b3%d1%80%d0%b0%d0%b4%d0%b8%d0%b5%d0%bd%d1%82%d0%bd%d1%8b%d0%b9-%d0%b1%d1%83%d1%81%d1%82%d0%b8%d0%bd%d0%b3/)\n",
    "* [Пост про стекинг и блендинг от А.Дьяконова](https://alexanderdyakonov.wordpress.com/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/)\n",
    "* [Еще одна библиотека на питоне для ансамблей](http://ml-ensemble.com/info/tutorials/start.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "347px",
    "width": "253px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "917px",
    "left": "0px",
    "right": "1572px",
    "top": "139px",
    "width": "348px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "40px",
   "left": "816px",
   "right": "38.6667px",
   "top": "0px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
